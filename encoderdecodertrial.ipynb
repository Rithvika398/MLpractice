{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "encoderdecodertrial.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rithvika398/MLpractice/blob/master/encoderdecodertrial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "w5d5DHdrEkXm",
        "colab_type": "code",
        "outputId": "7bc5d8d3-9423-49b1-d01a-bf81dedc1bb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np #matrix math \n",
        "import tensorflow as tf #machine learning\n",
        "#import helpers #for formatting data into batches and generating random sequence data\n",
        "\n",
        "tf.reset_default_graph() #Clears the default graph stack and resets the global default graph.\n",
        "sess = tf.InteractiveSession() #initializes a tensorflow session"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "5z_wvmoZGyko",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "def batchfunc(inputs, max_sequence_length=None):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        inputs:\n",
        "            list of sentences (integer lists)\n",
        "        max_sequence_length:\n",
        "            integer specifying how large should `max_time` dimension be.\n",
        "            If None, maximum sequence length would be used\n",
        "    \n",
        "    Outputs:\n",
        "        inputs_time_major:\n",
        "            input sentences transformed into time-major matrix \n",
        "            (shape [max_time, batch_size]) padded with 0s\n",
        "        sequence_lengths:\n",
        "            batch-sized list of integers specifying amount of active \n",
        "            time steps in each input sequence\n",
        "    \"\"\"\n",
        "    \n",
        "    sequence_lengths = [len(seq) for seq in inputs]\n",
        "    batch_size = len(inputs)\n",
        "    \n",
        "    if max_sequence_length is None:\n",
        "        max_sequence_length = max(sequence_lengths)\n",
        "    \n",
        "    inputs_batch_major = np.zeros(shape=[batch_size, max_sequence_length], dtype=np.int32) # == PAD\n",
        "    \n",
        "    for i, seq in enumerate(inputs):\n",
        "        for j, element in enumerate(seq):\n",
        "            inputs_batch_major[i, j] = element\n",
        "\n",
        "    # [batch_size, max_time] -> [max_time, batch_size]\n",
        "    inputs_time_major = inputs_batch_major.swapaxes(0, 1)\n",
        "\n",
        "    return inputs_time_major, sequence_lengths\n",
        "\n",
        "\n",
        "def random_sequences(length_from, length_to,\n",
        "                     vocab_lower, vocab_upper,\n",
        "                     batch_size):\n",
        "    \"\"\" Generates batches of random integer sequences,\n",
        "        sequence length in [length_from, length_to],\n",
        "        vocabulary in [vocab_lower, vocab_upper]\n",
        "    \"\"\"\n",
        "    if length_from > length_to:\n",
        "            raise ValueError('length_from > length_to')\n",
        "\n",
        "    def random_length():\n",
        "        if length_from == length_to:\n",
        "            return length_from\n",
        "        return np.random.randint(length_from, length_to + 1)\n",
        "    \n",
        "    while True:\n",
        "        yield [\n",
        "            np.random.randint(low=vocab_lower,\n",
        "                              high=vocab_upper,\n",
        "                              size=random_length()).tolist()\n",
        "            for _ in range(batch_size)\n",
        "        ]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LwpEg8b5EoeY",
        "colab_type": "code",
        "outputId": "fdcd992c-57c8-4afc-ee00-ab025b0b5dc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.12.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "metadata": {
        "id": "7TV_dZQePL2N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#First critical thing to decide: vocabulary size.\n",
        "#Dynamic RNN models can be adapted to different batch sizes \n",
        "#and sequence lengths without retraining \n",
        "#(e.g. by serializing model parameters and Graph definitions via tf.train.Saver), \n",
        "#but changing vocabulary size requires retraining the model.\n",
        "\n",
        "PAD = 0\n",
        "EOS = 1\n",
        "\n",
        "vocab_size = 10\n",
        "input_embedding_size = 20 #character length\n",
        "\n",
        "encoder_hidden_units = 20 #num neurons\n",
        "decoder_hidden_units = encoder_hidden_units * 2 #in original paper, they used same number of neurons for both encoder\n",
        "#and decoder, but we use twice as many so decoded output is different, the target value is the original input \n",
        "#in this example"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7wTdnMZ2P_zW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#input placehodlers\n",
        "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.string, name='encoder_inputs')\n",
        "#contains the lengths for each of the sequence in the batch, we will pad so all the same\n",
        "#if you don't want to pad, check out dynamic memory networks to input variable length sequences\n",
        "encoder_inputs_length = tf.placeholder(shape=(None,), dtype=tf.string, name='encoder_inputs_length')\n",
        "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.string, name='decoder_targets')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1DsqIAMaQToc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
        "\n",
        "#this thing could get huge in a real world application\n",
        "encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b-4Jmx5fSlDg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops.rnn_cell import LSTMCell, LSTMStateTuple\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x9iaO_tySsC4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder_cell = LSTMCell(encoder_hidden_units)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jm5uoyJMSxS-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "((encoder_fw_outputs,\n",
        "  encoder_bw_outputs),\n",
        " (encoder_fw_final_state,\n",
        "  encoder_bw_final_state)) = (\n",
        "    tf.nn.bidirectional_dynamic_rnn(cell_fw=encoder_cell,\n",
        "                                    cell_bw=encoder_cell,\n",
        "                                    inputs=encoder_inputs_embedded,\n",
        "                                    sequence_length=encoder_inputs_length,\n",
        "                                    dtype=tf.float32, time_major=True)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PGvvq25pC8RM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "92157f97-88c9-4e7b-ca35-d24582c76975"
      },
      "cell_type": "code",
      "source": [
        "encoder_fw_outputs\n"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'bidirectional_rnn_1/fw/fw/TensorArrayStack/TensorArrayGatherV3:0' shape=(?, ?, 20) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "metadata": {
        "id": "vXfi_cfYFp3-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3702c940-ea55-4f5d-bb4d-979c108eaaff"
      },
      "cell_type": "code",
      "source": [
        "encoder_bw_outputs\n"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'ReverseSequence:0' shape=(?, ?, 20) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "metadata": {
        "id": "tA5x8VCcFtXr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e9b443f0-bffe-4c9f-bbd4-925c6860f516"
      },
      "cell_type": "code",
      "source": [
        "encoder_fw_final_state\n"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMStateTuple(c=<tf.Tensor 'bidirectional_rnn_1/fw/fw/while/Exit_3:0' shape=(?, 20) dtype=float32>, h=<tf.Tensor 'bidirectional_rnn_1/fw/fw/while/Exit_4:0' shape=(?, 20) dtype=float32>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "metadata": {
        "id": "9-BmMQGXFwa3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c98f6b44-1c88-4dcd-bb95-60c903fafdeb"
      },
      "cell_type": "code",
      "source": [
        "encoder_bw_final_state\n"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMStateTuple(c=<tf.Tensor 'bidirectional_rnn_1/bw/bw/while/Exit_3:0' shape=(?, 20) dtype=float32>, h=<tf.Tensor 'bidirectional_rnn_1/bw/bw/while/Exit_4:0' shape=(?, 20) dtype=float32>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "metadata": {
        "id": "w1hT158OF0NN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Concatenates tensors along one dimension.\n",
        "encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\n",
        "\n",
        "#letters h and c are commonly used to denote \"output value\" and \"cell state\". \n",
        "#http://colah.github.io/posts/2015-08-Understanding-LSTMs/ \n",
        "#Those tensors represent combined internal state of the cell, and should be passed together. \n",
        "\n",
        "encoder_final_state_c = tf.concat(\n",
        "    (encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
        "\n",
        "encoder_final_state_h = tf.concat(\n",
        "    (encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
        "\n",
        "#TF Tuple used by LSTM Cells for state_size, zero_state, and output state.\n",
        "encoder_final_state = LSTMStateTuple(\n",
        "    c=encoder_final_state_c,\n",
        "    h=encoder_final_state_h\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rfbCMqhdLcDU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decoder_cell = LSTMCell(decoder_hidden_units)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FWyjXTBpLlLZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder_max_time, batch_size = tf.unstack(tf.shape(encoder_inputs))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1rQaSCC7Ln_f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decoder_lengths = encoder_inputs_length + 3\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YpLCpMYpLr4K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#manually specifying since we are going to implement attention details for the decoder in a sec\n",
        "#weights\n",
        "W = tf.Variable(tf.random_uniform([decoder_hidden_units, vocab_size], -1, 1), dtype=tf.float32)\n",
        "#bias\n",
        "b = tf.Variable(tf.zeros([vocab_size]), dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-JH64tFQLws8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#create padded inputs for the decoder from the word embeddings\n",
        "\n",
        "#were telling the program to test a condition, and trigger an error if the condition is false.\n",
        "assert EOS == 1 and PAD == 0\n",
        "\n",
        "eos_time_slice = tf.ones([batch_size], dtype=tf.int32, name='EOS')\n",
        "pad_time_slice = tf.zeros([batch_size], dtype=tf.int32, name='PAD')\n",
        "\n",
        "#retrieves rows of the params tensor. The behavior is similar to using indexing with arrays in numpy\n",
        "eos_step_embedded = tf.nn.embedding_lookup(embeddings, eos_time_slice)\n",
        "pad_step_embedded = tf.nn.embedding_lookup(embeddings, pad_time_slice)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l4xxENRiL0gH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#manually specifying loop function through time - to get initial cell state and input to RNN\n",
        "#normally we'd just use dynamic_rnn, but lets get detailed here with raw_rnn\n",
        "\n",
        "#we define and return these values, no operations occur here\n",
        "def loop_fn_initial():\n",
        "    initial_elements_finished = (0 >= decoder_lengths)  # all False at the initial step\n",
        "    #end of sentence\n",
        "    initial_input = eos_step_embedded\n",
        "    #last time steps cell state\n",
        "    initial_cell_state = encoder_final_state\n",
        "    #none\n",
        "    initial_cell_output = None\n",
        "    #none\n",
        "    initial_loop_state = None  # we don't need to pass any additional information\n",
        "    return (initial_elements_finished,\n",
        "            initial_input,\n",
        "            initial_cell_state,\n",
        "            initial_cell_output,\n",
        "            initial_loop_state)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CUzjsY8jL4zJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#attention mechanism --choose which previously generated token to pass as input in the next timestep\n",
        "def loop_fn_transition(time, previous_output, previous_state, previous_loop_state):\n",
        "\n",
        "    \n",
        "    def get_next_input():\n",
        "        #dot product between previous ouput and weights, then + biases\n",
        "        output_logits = tf.add(tf.matmul(previous_output, W), b)\n",
        "        #Logits simply means that the function operates on the unscaled output of \n",
        "        #earlier layers and that the relative scale to understand the units is linear. \n",
        "        #It means, in particular, the sum of the inputs may not equal 1, that the values are not probabilities \n",
        "        #(you might have an input of 5).\n",
        "        #prediction value at current time step\n",
        "        \n",
        "        #Returns the index with the largest value across axes of a tensor.\n",
        "        prediction = tf.argmax(output_logits, axis=1)\n",
        "        #embed prediction for the next input\n",
        "        next_input = tf.nn.embedding_lookup(embeddings, prediction)\n",
        "        return next_input\n",
        "    \n",
        "    \n",
        "    elements_finished = (time >= decoder_lengths) # this operation produces boolean tensor of [batch_size]\n",
        "                                                  # defining if corresponding sequence has ended\n",
        "\n",
        "    \n",
        "    \n",
        "    #Computes the \"logical and\" of elements across dimensions of a tensor.\n",
        "    finished = tf.reduce_all(elements_finished) # -> boolean scalar\n",
        "    #Return either fn1() or fn2() based on the boolean predicate pred.\n",
        "    input = tf.cond(finished, lambda: pad_step_embedded, get_next_input)\n",
        "    \n",
        "    #set previous to current\n",
        "    state = previous_state\n",
        "    output = previous_output\n",
        "    loop_state = None\n",
        "\n",
        "    return (elements_finished, \n",
        "            input,\n",
        "            state,\n",
        "            output,\n",
        "            loop_state)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7bfM9-hiL_3F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def loop_fn(time, previous_output, previous_state, previous_loop_state):\n",
        "    if previous_state is None:    # time == 0\n",
        "        assert previous_output is None and previous_state is None\n",
        "        return loop_fn_initial()\n",
        "    else:\n",
        "        return loop_fn_transition(time, previous_output, previous_state, previous_loop_state)\n",
        "\n",
        "#Creates an RNN specified by RNNCell cell and loop function loop_fn.\n",
        "#This function is a more primitive version of dynamic_rnn that provides more direct access to the \n",
        "#inputs each iteration. It also provides more control over when to start and finish reading the sequence, \n",
        "#and what to emit for the output.\n",
        "#ta = tensor array\n",
        "decoder_outputs_ta, decoder_final_state, _ = tf.nn.raw_rnn(decoder_cell, loop_fn)\n",
        "decoder_outputs = decoder_outputs_ta.stack()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lj2HmYgEMDyq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3605094c-114f-470e-d329-62a9b3aa39f4"
      },
      "cell_type": "code",
      "source": [
        "decoder_outputs\n"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'TensorArrayStack/TensorArrayGatherV3:0' shape=(?, ?, 40) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "metadata": {
        "id": "O3VhGeofMGyl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#to convert output to human readable prediction\n",
        "#we will reshape output tensor\n",
        "\n",
        "#Unpacks the given dimension of a rank-R tensor into rank-(R-1) tensors.\n",
        "#reduces dimensionality\n",
        "decoder_max_steps, decoder_batch_size, decoder_dim = tf.unstack(tf.shape(decoder_outputs))\n",
        "#flettened output tensor\n",
        "decoder_outputs_flat = tf.reshape(decoder_outputs, (-1, decoder_dim))\n",
        "#pass flattened tensor through decoder\n",
        "decoder_logits_flat = tf.add(tf.matmul(decoder_outputs_flat, W), b)\n",
        "#prediction vals\n",
        "decoder_logits = tf.reshape(decoder_logits_flat, (decoder_max_steps, decoder_batch_size, vocab_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "movfmUPBMLCv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decoder_prediction = tf.argmax(decoder_logits, 2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rBA1FpEQMOYn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#cross entropy loss\n",
        "#one hot encode the target values so we don't rank just differentiate\n",
        "stepwise_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
        "    labels=tf.one_hot(decoder_targets, depth=vocab_size, dtype=tf.float32),\n",
        "    logits=decoder_logits,\n",
        ")\n",
        "\n",
        "#loss function\n",
        "loss = tf.reduce_mean(stepwise_cross_entropy)\n",
        "#train it \n",
        "train_op = tf.train.AdamOptimizer().minimize(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yfx1cwemMSLt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess.run(tf.global_variables_initializer())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DoZrk91CMW0Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "a0c090e8-0278-4b0c-d9fd-f43ccba2f5c8"
      },
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "\n",
        "batches = random_sequences(length_from=3, length_to=8,\n",
        "                                   vocab_lower=2, vocab_upper=10,\n",
        "                                   batch_size=batch_size)\n",
        "\n",
        "print('head of the batch:')\n",
        "for seq in next(batches)[:10]:\n",
        "    print(seq)"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "head of the batch:\n",
            "[2, 2, 3, 3, 5]\n",
            "[2, 2, 6]\n",
            "[5, 6, 5, 5, 7, 7]\n",
            "[3, 4, 3, 4, 6]\n",
            "[7, 9, 9]\n",
            "[8, 4, 3]\n",
            "[3, 2, 6, 3, 6]\n",
            "[7, 3, 3, 4, 6]\n",
            "[7, 2, 8, 8, 4, 8, 7, 9]\n",
            "[9, 8, 9, 8, 7, 7, 8, 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-0U_awRbMqgD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def next_feed():\n",
        "    batch = next(batches)\n",
        "    encoder_inputs_, encoder_input_lengths_ = batchfunc(batch)\n",
        "    decoder_targets_, _ = batchfunc(\n",
        "        [(sequence) + [EOS] + [PAD] * 2 for sequence in batch]\n",
        "    )\n",
        "    return {\n",
        "        encoder_inputs: encoder_inputs_,\n",
        "        encoder_inputs_length: encoder_input_lengths_,\n",
        "        decoder_targets: decoder_targets_,\n",
        "    }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uPjTewqLwfu9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def data():\n",
        "  l= ['KA 00QD 9818', 'KA 00JE 5855', 'KA 00XR 7470', 'KA 00FB 4471', 'KA 00HD 3350', 'KA 00SQ 6691', 'KA 00KY 1656', 'KA 00OB 6248', 'KA 00DH 3249', 'KA 00NU 6548', 'KA 00TD 1234', 'KA 00HF 9965', 'KA 00UA 2870', 'KA 00AI 7186', 'KA 00KH 1017', 'KA 00SQ 5150', 'KA 00CF 1394', 'KA 00RU 4853', 'KA 00QZ 3878', 'KA 00NK 5513', 'KA 00MK 6173', 'KA 00FE 9960', 'KA 00BM 5125', 'KA 00EF 7988', 'KA 00PS 3872', 'KA 00GL 6489', 'KA 00KI 9047', 'KA 00QJ 1353', 'KA 00SW 4104', 'KA 00JN 7287', 'KA 00RK 9466', 'KA 00TY 1971', 'KA 00HT 2756', 'KA 00TJ 6252', 'KA 00QF 6317', 'KA 00XY 8464', 'KA 00FL 6326', 'KA 00WP 3812', 'KA 00FG 6695', 'KA 00PJ 4524', 'KA 00IS 8354', 'KA 00OM 5336', 'KA 00OH 8873', 'KA 00YD 6503', 'KA 00IP 9105', 'KA 00BU 4907', 'KA 00XH 4337', 'KA 00NL 9638', 'KA 00XY 7673', 'KA 00NL 5628', 'KA 00EZ 6035', 'KA 00TF 6067', 'KA 00IX 6514', 'KA 00VV 2762', 'KA 00NU 3907', 'KA 00KZ 2003', 'KA 00EP 3653', 'KA 00PK 5655', 'KA 00WX 2753', 'KA 00JN 4884', 'KA 00HY 7818', 'KA 00HJ 8002', 'KA 00VN 1024', 'KA 00VS 8590', 'KA 00GX 1006', 'KA 00QY 2869', 'KA 00DU 7176', 'KA 00SE 8715', 'KA 00IW 8819', 'KA 00MI 5216', 'KA 00YN 3990', 'KA 00UU 3933', 'KA 00GV 4125', 'KA 00IL 7715', 'KA 00KG 5668', 'KA 00WA 2558', 'KA 00AL 7440', 'KA 00ZX 2307', 'KA 00KI 2703', 'KA 00VT 8125', 'KA 00PD 3692', 'KA 00GQ 4829', 'KA 00IT 6652', 'KA 00QL 7479', 'KA 00OH 8055', 'KA 00EJ 9298', 'KA 00NC 8814', 'KA 00EN 3528', 'KA 00RZ 4556', 'KA 00PY 4990', 'KA 00AO 1558', 'KA 00QN 9085', 'KA 00BU 1224', 'KA 00TX 3495', 'KA 00WQ 4007', 'KA 00VQ 5644', 'KA 00YE 4571', 'KA 00AH 6746', 'KA 00ZO 2840', 'KA 00RS 9755']\n",
        "  encoder_inputs_, encoder_input_lengths_=batchfunc(l)\n",
        "  decoder_targets_, _ = batchfunc(\n",
        "        [(sequence) + [EOS] + [PAD] * 2 for sequence in l]\n",
        "  )\n",
        "  return {\n",
        "        encoder_inputs: encoder_inputs_,\n",
        "        encoder_inputs_length: encoder_input_lengths_,\n",
        "        decoder_targets: decoder_targets_,\n",
        "    }\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2_rUDlcFMxo8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss_track = []\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xoxMSADIM17a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 823
        },
        "outputId": "4453f29e-f4b1-4d3d-8225-5e8c0584dacd"
      },
      "cell_type": "code",
      "source": [
        "max_batches = 3001\n",
        "batches_in_epoch = 1000\n",
        "\n",
        "try:\n",
        "    for batch in range(max_batches):\n",
        "        fd = next_feed()\n",
        "        _, l = sess.run([train_op, loss], fd)\n",
        "        loss_track.append(l)\n",
        "\n",
        "        if batch == 0 or batch % batches_in_epoch == 0:\n",
        "            print('batch {}'.format(batch))\n",
        "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
        "            predict_ = sess.run(decoder_prediction, fd)\n",
        "            for i, (inp, pred) in enumerate(zip(fd[encoder_inputs].T, predict_.T)):\n",
        "                print('  sample {}:'.format(i + 1))\n",
        "                print('    input     > {}'.format(inp))\n",
        "                print('    predicted > {}'.format(pred))\n",
        "                if i >= 2:\n",
        "                    break\n",
        "            print()\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print('training interrupted')"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch 0\n",
            "  minibatch loss: 0.011218609288334846\n",
            "  sample 1:\n",
            "    input     > [4 9 5 0 0 0 0 0]\n",
            "    predicted > [4 9 5 1 0 0 0 0 0 0 0]\n",
            "  sample 2:\n",
            "    input     > [7 8 2 4 6 8 5 5]\n",
            "    predicted > [7 8 2 4 6 8 5 5 1 0 0]\n",
            "  sample 3:\n",
            "    input     > [8 4 7 0 0 0 0 0]\n",
            "    predicted > [8 4 7 1 0 0 0 0 0 0 0]\n",
            "\n",
            "batch 1000\n",
            "  minibatch loss: 0.007707471027970314\n",
            "  sample 1:\n",
            "    input     > [6 6 3 9 0 0 0 0]\n",
            "    predicted > [6 6 3 9 1 0 0 0 0 0 0]\n",
            "  sample 2:\n",
            "    input     > [6 5 6 8 6 0 0 0]\n",
            "    predicted > [6 5 6 8 6 1 0 0 0 0 0]\n",
            "  sample 3:\n",
            "    input     > [2 2 5 7 3 4 7 2]\n",
            "    predicted > [2 2 5 7 3 4 7 2 1 0 0]\n",
            "\n",
            "batch 2000\n",
            "  minibatch loss: 0.005945148877799511\n",
            "  sample 1:\n",
            "    input     > [5 8 6 0 0 0 0 0]\n",
            "    predicted > [5 8 6 1 0 0 0 0 0 0 0]\n",
            "  sample 2:\n",
            "    input     > [7 7 6 6 0 0 0 0]\n",
            "    predicted > [7 7 6 6 1 0 0 0 0 0 0]\n",
            "  sample 3:\n",
            "    input     > [3 5 2 9 8 0 0 0]\n",
            "    predicted > [3 5 2 9 8 1 0 0 0 0 0]\n",
            "\n",
            "batch 3000\n",
            "  minibatch loss: 0.008481381461024284\n",
            "  sample 1:\n",
            "    input     > [8 9 3 2 4 6 7 0]\n",
            "    predicted > [8 9 3 2 4 6 7 1 0 0 0]\n",
            "  sample 2:\n",
            "    input     > [2 9 5 3 5 0 0 0]\n",
            "    predicted > [2 9 5 3 5 1 0 0 0 0 0]\n",
            "  sample 3:\n",
            "    input     > [2 8 5 9 3 0 0 0]\n",
            "    predicted > [2 8 5 9 3 1 0 0 0 0 0]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kIpBTo9hM6L-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "291c87d7-c1f3-489f-dbb2-891152414933"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(loss_track)\n",
        "print('loss {:.4f} after {} examples (batch_size={})'.format(loss_track[-1], len(loss_track)*batch_size, batch_size))"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss 0.1317 after 300100 examples (batch_size=100)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fa90ce5aba8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Z14UnvfY0TGx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "outputId": "bfca2925-091e-498c-ec0b-9c9605158b8f"
      },
      "cell_type": "code",
      "source": [
        "max_batches = 3001\n",
        "batches_in_epoch = 1000\n",
        "\n",
        "try:\n",
        "    for batch in range(max_batches):\n",
        "        fd = data()\n",
        "        _, l = sess.run([train_op, loss], fd)\n",
        "        loss_track.append(l)\n",
        "\n",
        "        if batch == 0 or batch % batches_in_epoch == 0:\n",
        "            print('batch {}'.format(batch))\n",
        "            print('  minibatch loss: {}'.format(sess.run(loss, fd)))\n",
        "            predict_ = sess.run(decoder_prediction, fd)\n",
        "            for i, (inp, pred) in enumerate(zip(fd[encoder_inputs].T, predict_.T)):\n",
        "                print('  sample {}:'.format(i + 1))\n",
        "                print('    input     > {}'.format(inp))\n",
        "                print('    predicted > {}'.format(pred))\n",
        "                if i >= 2:\n",
        "                    break\n",
        "            print()\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print('training interrupted')"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-157-03c6dfdedb9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss_track\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-154-b5f3d34aac64>\u001b[0m in \u001b[0;36mdata\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'KA 00QD 9818'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KA 00JE 5855'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KA 00XR 7470'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KA 00FB 4471'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KA 00HD 3350'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KA 00SQ 6691'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KA 00KY 1656'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KA 00OB 6248'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KA 00DH 3249'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KA 00NU 6548'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KA 00TD 1234'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KA 00HF 9965'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KA 00UA 2870'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KA 00AI 7186'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KA 00KH 1017'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KA 00SQ 5150'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KA 00CF 1394'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KA 00RU 4853'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KA 00QZ 3878'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KA 00NK 5513'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KA 00MK 6173'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KA 00FE 9960'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KA 00BM 5125'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KA 00EF 7988'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KA 00PS 3872'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KA 00GL 6489'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KA 00KI 9047'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KA 00QJ 1353'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KA 00SW 4104'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KA 00JN 7287'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KA 00RK 9466'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KA 00TY 1971'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KA 00HT 2756'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KA 00TJ 6252'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KA 00QF 6317'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KA 00XY 8464'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KA 00FL 6326'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'KA 00WP 3812'\u001b[0m\u001b[0;34m,\u001b[0...\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mencoder_inputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_input_lengths_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatchfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   decoder_targets_, _ = batchfunc(\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mEOS\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mPAD\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msequence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-119-18958beb4110>\u001b[0m in \u001b[0;36mbatchfunc\u001b[0;34m(inputs, max_sequence_length)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0minputs_batch_major\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# [batch_size, max_time] -> [max_time, batch_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'K'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "_FaCN6Dc2aFg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}